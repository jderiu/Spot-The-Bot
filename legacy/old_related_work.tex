\section{Related Work}
\label{sota}

Nowadays there exist many methods to evaluate dialogue systems, both automatic and human-based, but unfortunately there is no single evaluation metric that is widely agreed upon the scientific community \cite{deriu2019survey}. Task oriented dialogues can be evaluated based on whether the task is successfully accomplished, but open domain chit-chat systems, which do not have a precise goal, are more challenging to evaluate.
Automatic evaluation metrics on chit-chat systems are problematic and are known to correlate poorly with human ratings~\cite{liu-etal-2016-evaluate,lowe-etal-2017-towards}. 
A widely used alternative is to perform a human based evaluation, where annotators are asked to rate the quality of dialogues. Human based evaluation is usually classified into single-turn or multi-turn evaluation. Single-turn evaluation, where each turn of the dialogue is evaluated in isolation, has the benefit of being simple, and there are automatic single-turn metrics that correlate with humans scores~\cite{mehri2020usr}.
 However, single-turn evaluation fails to capture the quality of the conversation as a whole. For instance, a system that tend to produce repeated answers cat obtain a high single-turn score, albeit a low multi-turn one. In multi-turn evaluation humans evaluate complete dialogues, and is often measured using aggregate scores like Likert scores. 
However, Likert scores are known to suffer from high annotation variance, and are prone to order effects~\cite{amidei-etal-2019-use}. In any case, human based evaluation is expensive and time consuming, and results are difficult to compare across different studies \cite{van-der-lee-etal-2019-best,amidei2019agreement}. 
\citet{venkatesh2018evaluating} performed a study about how to evaluate dialogue systems at the Alexa Prize and compare results with those from humans' ratings. They proposed to measure domain coverage, coherence, topical diversity and conversational depth using different metrics. Some of these metrics are evaluated automatically, while others require humans. The authors also propose a unified measure that aggregate the metrics to obtain a global score. Compared with human ratings, this measure shows a high correlation of $0.66-0.7$. However, it is unclear how to apply the proposed method for new systems over the same dataset. Besides, the unified measure requires collecting several human annotations (for different features) and evaluating several systems.

Evaluation is usually performed on dialogues involving different entities. Reference datasets often consists of dialogues between two humans, but building such datasets is very costly and time consuming. Besides, humans are often asked to also rate the dialogues, which increases the cognitive strain posed on them~\cite{SCHMITT201512}. Collecting human bot conversations is less time costly, but the quality is often low. For instance, in the ConvAI2 challenge half of the collected human-bot conversations were discarded due to their low quality~\cite{convai12}. 
An alternative is to evaluate bots that talk to themselves~\cite{ghandeharioun2019approximating,li2019acuteeval,deriu-cieliebak-2019-towards}. In ACUTE-EVAL~\cite{li2019acuteeval} the authors evaluate bots that are self-chatting, and show that the results are comparable to human-bot chats. ACUTE-EVAL is similar to our method and also compares pairs of systems. However, ACUTE-EVAL requires humans to read two complete dialogues, while our method proposes to read a single dialogue until the user can make a decision. Besides, it does not provide a single measure per system.
\emph{Spot The Bot} is reminiscent of the Turing test, as the dialogue systems are evaluated based on their ability to mimic human behaviour. The Turing test has been criticized as a way to identify intelligence in NLP systems. For instance, \citet{bender-2020-acl} argue that a system may fool a user into believing it is human, and yet this does not prove that the system understands the meaning of the conversation they are having. However, in this paper we claim that \emph{failing} the test is a valid indicator that helps discriminate among bots that are competing with each other. Besides, we presume that eventually all the bots will fail the test, and hence we also collect a time component to record how much time it took to the bot to cheat the human.


% \section{Related Work}
% *TODO show that related work often suffers from reliability (low agreements between annotators), poor quality of human-bot convos (cf. convai2 threw away half their conversations), huge amount of conversations needed, cost and time (creating human-bot convos, many annotations on them).
% *TODO discuss Turing test and Alexa challenge (and ConvAI2 challenge).
% *TODO discuss ACUTEval from Facebok
% *TODO: include BERTScore?


% Dialogue systems are usually evaluated in terms of quality and diversity. There are two main approaches: automatic and human-based evaluations. Automatic methods are based on evaluation metrics from automatic summarization and machine translation like BLEU \cite{papineni-etal-2002-bleu} and ROUGE \cite{lin-2004-rouge}, focus on measuring the overlap with a reference dialogue. Then, correct dialogues with a low overlapping receive a low score, what is know as the one-to-many problem  \cite{zhao-etal-2017-learning}. Besides, models producing wrong responses with probable words could be over-rewarded  \cite{li-etal-2016-deep}. Thus, while automatic evaluation is a long-term objective that would benefit the development of new systems, they usually disagree with human evaluations \cite{liu-etal-2016-evaluate,lowe-etal-2017-towards}.

% An alternative to mitigate the one-to-many problem is to produce datasets with multiple references. Additional references can be created automatically \cite{galley-etal-2015-deltableu,sordoni-etal-2015-neural}, which does not reflect diversity, or using more humans \cite{Sugiyama2019}, what increase the annotation cost. Then, given a metric, the multiple-reference approach selects the highest score resulting from comparing the output with all the references. This simple approach has shown a higher correlation with human judgments than evaluations based on a single reference \cite{gupta-etal-2019-investigating}. Nevertheless, this approach requires a higher effort in creating datasets and does not cover all the possible correct sentences.

% Given the open nature of dialogues, it might be unfair to evaluate systems against a reference dialogue. This is why \citet{mehri2020usr} propose the UnSupervised and Reference free (USR) evaluation metric. USR measures properties of dialogues like being natural, keeping context or showing knowledge, without comparing them with a reference. For this purpose, USR uses a RoBERTa model \cite{DBLP:journals/corr/abs-1907-11692} fine-tuned for each property. Then, all the properties are combined into a single score using a regression model trained with human annotations. The final score shows a high correlation with human scores. Besides, this measure can include more properties and other ways of combining them.

% On the other hand, human evaluations are expensive and time-consuming, and results are difficult to compare across different studies [REF]. There are two kinds of performing human evaluations: (1) single-turn, where the human  scores just one turn of the dialogue and thus the method is faster but cannot capture the global dimension of the dialogue and; (2) multi-turn, where the human evaluates the whole dialogue. [*TODO include comments about the low agreement and high bias]]. * TODO: use of Likert scores

% There is another trend that tries to combine human and automatic evaluations given that human evaluations do not usually capture  diversity, but encourages quality, and automatic evaluations fail to measure quality but penalize the lack of diversity. Human Unified with Statistical Evaluation (HUSE) combines both approaches by measuring error rates of the model and combining them into a single score able to evaluate both quality and diversity \cite{hashimoto-etal-2019-unifying}.

% In order to overcome the annotator bias of common human evaluations, \citet{li2019acuteeval} have proposed an evaluation named ACUTE-EVAL. ACUTE-EVAL is based on comparing systems by pairs and avoiding humans to give scores. More in detail, several humans have to select between two systems with respect to two different dialogues produced by those systems conversing with humans. The selection is based on a single quality like engaging, interesting, knowledgeable or humanness. Then, given a set of systems to be evaluated, the method consists of running several trials comparing each pair of systems and return for each pair the percentage of winning matches according to each quality.  From this result, a ranking of systems can be given. ACUTE-EVAL is similar to our method since ACUTE-EVAL compares pairs of systems. However, ACUTE-EVAL requires humans to read two complete dialogues, while our method proposes to read a single dialogue until the user can make a decision. Besides, ACUTE-EVAL does not provide a single measure per system.

% \cite{venkatesh2018evaluating} performed a study about how to evaluate dialogue systems at the Alexa Prize \todo{Check if previous information about the Alexa prize has been given} and compare results with those from humans' ratings. They have available a large dataset containing millions of dialogues between humans and bots participating at the Alexa Prize, with each dialogue rated by the human who took part in the dialogue (in human evaluations, it is common that the annotator does not take part in the dialogue). The proposed to measure domain coverage, coherence, topical diversity and conversational depth using different metrics since the authors think these features are important as part of good dialogue. Some of these metrics are evaluated automatically, while others require humans. The authors also propose a unified measure combining all the others. This measure is based on giving a point to the best systems for each feature. Then, the final score for each system is the sum of all the points given to that system. Compared with human rating, this measure shows a correlation of 0,66-0,7. It is unclear how to apply the proposed method for new systems over the same dataset. Besides, the unified measure requires collecting several human annotations (for different features) and evaluating several systems.

% Current human evaluations are based on static data, without paying attention to interactivity, which is essential for evaluating diversity in dialogues. Since, it is difficult and expensive to evaluate systems in an interactive framework, \cite{ghandeharioun2019approximating} propose a method where a system converse with itself for a fixed number of turns. Then, the dialogue is evaluated taking into account metrics related to sentiment, semantics and user engagement. Finally, a function trained with human dialogues predicts a score taking into account those metrics. This final score shows a high correlation (0.7 Pearson correlation) with human scores.

% % thiziri added this:
% Another approach is to combine several models in a same dialog system. For instance, ConvLab \cite{lee-etal-2019-convlab} is an open-source toolkit that supports an end-to-end evaluation  of several state-of-the-art dialogue systems to compare a set of different approaches. ConvLab is based on Agents-Environments-Bodies (AEB) design architecture, where an agent refers to every instance of dialogue that can be either a human being or a simulated user, the environment is every evaluation component, and the body is the physical instance of the agent. A new release of this system, namely ConvLab-2, is proposed by \cite{zhu2020convlab}. ConvLab-2 performs a qualitative and quantitative analysis, extending its predecessor by integrating many recently proposed state-of-the-art dialogue models. In both ConvLab and ConvLab-2, each speaker in a conversation is considered as an agent. This aspect enables using multiple models for one component and supports multi-party conversations. The main limitation of this approach is that the upcoming interactions may be confused, and make it difficult to distinguish the bot from the human agent. Besides, this approach can lead to a disagreement between the human's decision and the simulator's decision. In \cite{sinha2020learning}, the authors propose the MAUDE model, an unsupervised unreferenced evaluation metric to evaluate the responses of a dialogue system, where no comparison between the responses is performed. This metric uses a set of pre-trained language models to compute semantic representations of utterances, including the temporal transitions between them. In MAUDE, an agent is provided with a set of utterances, called context, and is expected to provide a response. Every sequence is represented by a semantic vector where the component words are encoded using a contextual embedding model (BERT). This model focuses on the evaluation of generative neural dialogue models. However, it does not provide an explicit evaluation of the agent and its interaction with other agents (human beings or bots), and the comparison between different agents is not straightforward. Besides, it involves training text encoders using a noise contrastive estimation (NCE) to distinguish between valid dialogue responses and a set of negative examples, which requires a large amount of conversations to be trained.
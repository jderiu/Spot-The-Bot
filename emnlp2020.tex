%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\renewcommand{\arraystretch}{1.2}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Spot The Bot: A Framework for Conversational Dialogue System Evaluation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
%We introduce Spot The Bot, a framework for evaluating conversational dialogue systems (i.e.\ chat bots). Spot The Bot is a tournament between chat bots, whose goal it is to pass as humans. 
%The tournament is based on conversations between the competing bots, mixed with conversations between humans. The conversations are shown to human annotators who decide which participants are bots. 
%The bot that most frequently convinces human that it is human is ranked highest.

%We empirically show that Spot The bot is more time- and cost-efficient than related approaches and yields robust results. We apply the approach to three domains and several state-of-the-art chat bots to evaluate them.
\end{abstract}

\section{Introduction}\label{sec:intro}
\paragraph{Motivation.}
Evaluation is a crucial step in the development cycle of dialogue systems, which is very cost-and time-intensive and often not reproducible. This problem is especially pronounced for conversational dialogue systems (i.e. chit-chat bots), which work on open domains and do not have a clearly defined goal, which can be measured. This not only slows down the development of novel approaches but it also makes it hard to reliably recreate the evaluation. 
There are several open problems: 
\begin{itemize}
\item  Human-bot conversations are costly to obtain and they often suffer from low quality. Low quality dialogues are the result of the limited capabilities of the algorithms, which are typically specialized in the training domain, and the fact that humans do not adhere to the distribution in which the model has been trained. Furthermore, the cognitive strain on the annotators is increased when they have to both converse with the bot and then rate the conversation \cite{}.
\item Human ratings are less costly to obtain but they often suffer from low agreement scores. Furthermore, these ratings are hard to reproduce and not necessarily stable across different executions. 
\item Current offline evaluations do not take into account the multi-turn character of conversations. Often, the evaluation is performed on a static context (e.g. sampled from a test set) and a generated response utterance of the bot under consideration. 
\item There is a general lack of automation of the evaluation procedure. Most automated metrics do not correlate to human judgements \cite{foster-2008-automated}, or are unstable.
\end{itemize}
\paragraph{Claim.}
In this work, we present \emph{Spot The Bot}, a cost-and-time efficient evaluation methodology, which can be used to reliably discriminate different dialogue strategies. \emph{Spot The Bot} is based on the observation that conversational dialogue systems are trained on human-human conversations, and thus, the dialogue systems should be evaluated in their ability to mimic human behaviour. This idea is related to the Turing Test, where the goal is that a bot's behaviour is indistinguishable from human behaviour.
\emph{Spot The Bot} works by letting two bots converse with each other and letting crowdworkers decide for each entity in the conversation if it is a human or a bot. Since we assume that in most cases the bots are identified as such eventually, we add a time component so that the discriminating criterion is not whether a bot is mistaken for a human, but which dialogue system can disguise as a human the longest?

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{itemize}
\item We present an evaluation framework for dialogue systems that produces reliable, repeatable results. The framework is quicker and more cost-effective to run than related approaches, as it does not rely on human-bot conversations and generally requires fewer annotations.
\item The analysis within the framework allows to pin down features that contribute to a dialogue's system performance, enabling interpretable results. Furthermore, we show that disagreement between human annotators can be interpreted as a feature of a system's performance, rather than as a weakness in the evaluation approach. 
\item We apply the framework to three well-known domains and five (?)recently released state-of-the-art bots as well as common baselines to produce a stable ranking among them.
\item We release the framework as a ready-to-use tool for evaluating dialogue systems into which different systems can be plugged and compared.
\end{itemize}

\section{Related Work}

Nowadays there exist many methods to evaluate dialogue systems, both automatic and human-based, but unfortunately there is no single evaluation metric that is widely agreed upon the scientific community. Task oriented dialogues can be evaluated based on whether the task is successfully accomplished, but open domain chit-chat systems, which do not have a precise goal, are more challenging to evaluate.

% an extensive overview of eval methods for dialogue systems can be found in
% our survey: https://arxiv.org/pdf/1905.04071.pdf

Automatic evaluation metrics on chit-chat systems are problematic and are known to correlate poorly with human ratings~\cite{liu-etal-2016-evaluate,lowe-etal-2017-towards}. A widely used alternative is to perform a human based evaluation, where annotators are asked to rate the quality of dialogues. Human based evaluation is usually classified into single-turn or multi-turn evaluation. Single-turn evaluation, where each turn of the dialogue is evaluated in isolation, has the benefit of being simple, and there are automatic single-turn metrics that correlate with humans scores~\cite{mehri2020usr}. However, single-turn evaluation fails to capture the quality of the conversation as a whole. For instance, a system that tend to produce repeated answers cat obtain a high single-turn score, albeit a low multi-turn one. In multi-turn evaluation humans evaluate complete dialogues, and is often measured using aggregate scores like Likert scores. However, Likert scores are known to suffer from high annotation variance, and are prone to order effects~\cite{amidei-etal-2019-use}. In any case, human based evaluation is expensive and time consuming, and results are difficult to compare across different studies \cite{van-der-lee-etal-2019-best,amidei2019agreement}. 

\citet{venkatesh2018evaluating} performed a study about how to evaluate dialogue systems at the Alexa Prize and compare results with those from humans' ratings. They proposed to measure domain coverage, coherence, topical diversity and conversational depth using different metrics. Some of these metrics are evaluated automatically, while others require humans. The authors also propose a unified measure that aggregate the metrics to obtain a global score. Compared with human ratings, this measure shows a high correlation of $0.66-0.7$. However, it is unclear how to apply the proposed method for new systems over the same dataset. Besides, the unified measure requires collecting several human annotations (for different features) and evaluating several systems.

%A new release of this system, namely ConvLab-2, is proposed by \cite{zhu2020convlab}. ConvLab-2 performs a qualitative and quantitative analysis, extending its predecessor by integrating many recently proposed state-of-the-art dialogue models. In both ConvLab and ConvLab-2, each speaker in a conversation is considered as an agent. This aspect enables using multiple models for one component and supports multi-party conversations. The main limitation of this approach is that the upcoming interactions may be confused, and make it difficult to distinguish the bot from the human agent. Besides, this approach can lead to a disagreement between the human's decision and the simulator's decision. 

Evaluation is usually performed on dialogues involving different entities. Reference datasets often consists of dialogues between two humans, but building such datasets is very costly and time consuming. Besides, humans are often asked to also rate the dialogues, which increases the cognitive strain posed on them~\cite{SCHMITT201512}. Collecting human bot conversations is less time costly, but the quality is often low. For instance, in the ConvAI2 challenge half of the collected human bot conversations were discarded due to their low quality~\cite{convai12}. An alternative is to evaluate bots that talk to themselves~\cite{ghandeharioun2019approximating,li2019acuteeval,deriu-cieliebak-2019-towards}. In ACUTE-EVAL~\cite{li2019acuteeval} the authors evaluate bots that are self-chatting, and show that the results are comparable to human-bot chats. ACUTE-EVAL is similar to our method and also compares pairs of systems. However, ACUTE-EVAL requires humans to read two complete dialogues, while our method proposes to read a single dialogue until the user can make a decision. Besides, it does not provide a single measure per system.

\emph{Spot The Bot} is reminiscent of the Turing test, as the dialogue systems are evaluated based on their ability to mimic human behaviour. The Turing test has been criticized as a way to identify intelligence in NLP systems. For instance, \citet{bender-2020-acl} argue that a system may fool a user into believing it is human, and yet this does not prove that the system understands the meaning of the conversation they are having. However, in this paper we claim that \emph{failing} the test is a valid indicator that helps discriminate among bots that are competing with each other. Besides, we presume that eventually all the bots will fail the test, and hence we also collect a time component to record how much time it took to the bot to cheat the human.

% \section{Related Work}
% *TODO show that related work often suffers from reliability (low agreements between annotators), poor quality of human-bot convos (cf. convai2 threw away half their conversations), huge amount of conversations needed, cost and time (creating human-bot convos, many annotations on them).
% *TODO discuss Turing test and Alexa challenge (and ConvAI2 challenge).
% *TODO discuss ACUTEval from Facebok
% *TODO: include BERTScore?


% Dialogue systems are usually evaluated in terms of quality and diversity. There are two main approaches: automatic and human-based evaluations. Automatic methods are based on evaluation metrics from automatic summarization and machine translation like BLEU \cite{papineni-etal-2002-bleu} and ROUGE \cite{lin-2004-rouge}, focus on measuring the overlap with a reference dialogue. Then, correct dialogues with a low overlapping receive a low score, what is know as the one-to-many problem  \cite{zhao-etal-2017-learning}. Besides, models producing wrong responses with probable words could be over-rewarded  \cite{li-etal-2016-deep}. Thus, while automatic evaluation is a long-term objective that would benefit the development of new systems, they usually disagree with human evaluations \cite{liu-etal-2016-evaluate,lowe-etal-2017-towards}.

% An alternative to mitigate the one-to-many problem is to produce datasets with multiple references. Additional references can be created automatically \cite{galley-etal-2015-deltableu,sordoni-etal-2015-neural}, which does not reflect diversity, or using more humans \cite{Sugiyama2019}, what increase the annotation cost. Then, given a metric, the multiple-reference approach selects the highest score resulting from comparing the output with all the references. This simple approach has shown a higher correlation with human judgments than evaluations based on a single reference \cite{gupta-etal-2019-investigating}. Nevertheless, this approach requires a higher effort in creating datasets and does not cover all the possible correct sentences.

% Given the open nature of dialogues, it might be unfair to evaluate systems against a reference dialogue. This is why \citet{mehri2020usr} propose the UnSupervised and Reference free (USR) evaluation metric. USR measures properties of dialogues like being natural, keeping context or showing knowledge, without comparing them with a reference. For this purpose, USR uses a RoBERTa model \cite{DBLP:journals/corr/abs-1907-11692} fine-tuned for each property. Then, all the properties are combined into a single score using a regression model trained with human annotations. The final score shows a high correlation with human scores. Besides, this measure can include more properties and other ways of combining them.

% On the other hand, human evaluations are expensive and time-consuming, and results are difficult to compare across different studies [REF]. There are two kinds of performing human evaluations: (1) single-turn, where the human  scores just one turn of the dialogue and thus the method is faster but cannot capture the global dimension of the dialogue and; (2) multi-turn, where the human evaluates the whole dialogue. [*TODO include comments about the low agreement and high bias]]. * TODO: use of Likert scores

% There is another trend that tries to combine human and automatic evaluations given that human evaluations do not usually capture  diversity, but encourages quality, and automatic evaluations fail to measure quality but penalize the lack of diversity. Human Unified with Statistical Evaluation (HUSE) combines both approaches by measuring error rates of the model and combining them into a single score able to evaluate both quality and diversity \cite{hashimoto-etal-2019-unifying}.

% In order to overcome the annotator bias of common human evaluations, \citet{li2019acuteeval} have proposed an evaluation named ACUTE-EVAL. ACUTE-EVAL is based on comparing systems by pairs and avoiding humans to give scores. More in detail, several humans have to select between two systems with respect to two different dialogues produced by those systems conversing with humans. The selection is based on a single quality like engaging, interesting, knowledgeable or humanness. Then, given a set of systems to be evaluated, the method consists of running several trials comparing each pair of systems and return for each pair the percentage of winning matches according to each quality.  From this result, a ranking of systems can be given. ACUTE-EVAL is similar to our method since ACUTE-EVAL compares pairs of systems. However, ACUTE-EVAL requires humans to read two complete dialogues, while our method proposes to read a single dialogue until the user can make a decision. Besides, ACUTE-EVAL does not provide a single measure per system.

% \cite{venkatesh2018evaluating} performed a study about how to evaluate dialogue systems at the Alexa Prize \todo{Check if previous information about the Alexa prize has been given} and compare results with those from humans' ratings. They have available a large dataset containing millions of dialogues between humans and bots participating at the Alexa Prize, with each dialogue rated by the human who took part in the dialogue (in human evaluations, it is common that the annotator does not take part in the dialogue). The proposed to measure domain coverage, coherence, topical diversity and conversational depth using different metrics since the authors think these features are important as part of good dialogue. Some of these metrics are evaluated automatically, while others require humans. The authors also propose a unified measure combining all the others. This measure is based on giving a point to the best systems for each feature. Then, the final score for each system is the sum of all the points given to that system. Compared with human rating, this measure shows a correlation of 0,66-0,7. It is unclear how to apply the proposed method for new systems over the same dataset. Besides, the unified measure requires collecting several human annotations (for different features) and evaluating several systems.

% Current human evaluations are based on static data, without paying attention to interactivity, which is essential for evaluating diversity in dialogues. Since, it is difficult and expensive to evaluate systems in an interactive framework, \cite{ghandeharioun2019approximating} propose a method where a system converse with itself for a fixed number of turns. Then, the dialogue is evaluated taking into account metrics related to sentiment, semantics and user engagement. Finally, a function trained with human dialogues predicts a score taking into account those metrics. This final score shows a high correlation (0.7 Pearson correlation) with human scores.

% % thiziri added this:
% Another approach is to combine several models in a same dialog system. For instance, ConvLab \cite{lee-etal-2019-convlab} is an open-source toolkit that supports an end-to-end evaluation  of several state-of-the-art dialogue systems to compare a set of different approaches. ConvLab is based on Agents-Environments-Bodies (AEB) design architecture, where an agent refers to every instance of dialogue that can be either a human being or a simulated user, the environment is every evaluation component, and the body is the physical instance of the agent. A new release of this system, namely ConvLab-2, is proposed by \cite{zhu2020convlab}. ConvLab-2 performs a qualitative and quantitative analysis, extending its predecessor by integrating many recently proposed state-of-the-art dialogue models. In both ConvLab and ConvLab-2, each speaker in a conversation is considered as an agent. This aspect enables using multiple models for one component and supports multi-party conversations. The main limitation of this approach is that the upcoming interactions may be confused, and make it difficult to distinguish the bot from the human agent. Besides, this approach can lead to a disagreement between the human's decision and the simulator's decision. In \cite{sinha2020learning}, the authors propose the MAUDE model, an unsupervised unreferenced evaluation metric to evaluate the responses of a dialogue system, where no comparison between the responses is performed. This metric uses a set of pre-trained language models to compute semantic representations of utterances, including the temporal transitions between them. In MAUDE, an agent is provided with a set of utterances, called context, and is expected to provide a response. Every sequence is represented by a semantic vector where the component words are encoded using a contextual embedding model (BERT). This model focuses on the evaluation of generative neural dialogue models. However, it does not provide an explicit evaluation of the agent and its interaction with other agents (human beings or bots), and the comparison between different agents is not straightforward. Besides, it involves training text encoders using a noise contrastive estimation (NCE) to distinguish between valid dialogue responses and a set of negative examples, which requires a large amount of conversations to be trained.

\section{Spot The Bot}

In this section, we first provide an overview of the \emph{Spot The Bot} framework and then describe the individual steps of the evaluation process.

\subsection{Overview}
% figure
%----------------------------------------------------------------------------
\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Spot the bot - annotation example - sketch3.pdf}
    \caption{Overview of the Spot The Bot process. 1: A bot-bot conversation is segmented into sub-segments of different lengths (2, 3, and 5 exchanges). 2: These are show to distinct sets of annotators who judge whether each entity is a bot. 4: The winner is determined for each annotated segment. This process is repeated for all conversations between the competing bots. 5: Finally, wins are aggregated from all annotated segments for each bot to create a ranking. *TODO make this figure prettier; find better visual presentation.}
    \label{fig:example}
\end{figure*}
%----------------------------------------------------------------------------
\emph{Spot The Bot} employs a tournament among conversational dialogue systems (bots) to determine which performs the best at mimicking the conversational behaviour of humans. To measure the success of each bot, human crowdworkers are shown conversations between the competing bots, mixed with conversations between humans. The crowdworkers' task is then to determine for each participant in a conversation whether it is a human or a bot (or whether they are unsure). The bot that is most frequently annotated as being human, wins the tournament. Figure \ref{fig:example} provides an overview of the process for one conversation.

More formally, assume that there is a pool of $B$ bots $\{B_1, ..., B_B\}$, which is to be ranked, e.g.\ when a novel dialogue strategy is to be compared against existing strategies. For each pair of bots, a set of conversations is sampled by letting the bots talk to each other, where $S_{ij}$ denotes the set of conversations between $B_i$ and $B_j$. Each conversation is defined as a sequence of exchanges $e_0, ..., e_N$, where each exchange consists of two turns, one per entity $e_i = \{t_0^{e_i}, t_1^{e_i}\}$.

%\emph{Spot The Bot} employs an annotation task that lets human crowdworkers read a conversation and decide for each entity in the conversation if it is a human or a bot. 
%The bot-bot conversations are then shown to human crowdworkers who determine for each participant in a conversation whether it is a bot or a human.
Showing only bot-bot conversations to the crowdworkers raises two issues: First, many conversational dialogue systems are not yet human-like enough to fool humans. Second, the crowdworkers might soon realize that all conversations are among bots, thus, classifying all entities as bots. 
To alleviate the first issue, we add a time component under the assumption that disguising as humans in shorter conversations is achievable also for weaker bots. Hence, we segment the conversations into different stages: $e_0, ..., e_i$, as shown in Figure \ref{fig:example}. The bots are then differentiated not only by their ability to fool humans, but also by how long they are able to pass as a human (or at least are not spotted as a bot). For the second issue, we add conversations between humans into the pool. These serve two purposes: first, as distractors, so that the crowdworkers cannot classify all entities as bots, and second, as an upper bound for evaluating the human-like behaviour of the competing bots.
Additionally, the framework allows to rate specific features, which are often used in current human evaluation (e.g. fluency, appropriateness, ...). The framework then measures the influence of these features on the survival time of the bots. 
%Additionally, we ask the crowdworkers to state which bot performed better with regards to certain features: fluency, sensibleness, and specificity as defined in \cite{adiwardana2020towards}. 
To rank the bots, we apply two different analysis on the annotation outcome: pair-wise ranking (rank the bots based on pair-wise comparisons from the annotated conversations), and survival analysis (i.e. probability of survival at each exchange and the influence of the different features).

\subsection{Rating Process}\label{sec:setting}

\paragraph{Bot-Bot Conversations.} For each pair of bots $B_i$, $B_j$ in the pool, we sample $|S_{ij}|$ conversations. In order to prime the conversations and obtain some variety of dialogues in $S_{ij}$.Given a set of contexts in the test set, the conversations are primed by showing the context to the bots. However, the initial context is not shown to the crowdworkers during annotation, since they are written by humans and are not part of the bot output. %The length of the sampled conversation is based on the conversation lengths in the given domain. This is to avoid that the bots are spotted by simply looking at the conversation length. 
%\paragraph{Human-Bot Conversations.} Ideally, our approach works without relying on human-bot conversations, which are costly to obtain and often very noisy if not executed with care. We show that our approach works without the need of human-bot conversations, by comparing the results of the \emph{Spot The Bot} annotation procedure with and without human-bot conversations. For this, we collected human-bot conversations for each bot by letting members of our lab converse with them. We instructed them to adhere to the dialogue style of the respective domain and to avoid being adversarial. 
\paragraph{Segmentation.} In order to simulate the time component, we show the conversations at different stages $e_0, ..., e_i$, where we chose different sets of values for $i$ depending on the domain\footnote{We experimented with letting crowdworkers decide themselves when to decide the exchange at which they were sure that an entity in the conversation is a bot or a human. However, this approach required too much fine-tuning to elicit the desired behaviour, cf.\ Appendix \ref{sec:appendix_gamification}.}. Each segment of the same conversation is annotated by different annotators to avoid that one annotator sees different segments of the same conversation. We assign two workers per conversation segment to be able to analyze their agreement.
\paragraph{Annotation.} The generated bot-bot conversations and a set of randomly sampled human-human conversations are presented to crowdworkers. The crowdworkers are presented with a conversation segment and have to decide for each entity if it is a human or a bot. There is an option for "unsure" as well, since we want to avoid forcing random decisions when annotators are not confident.%are interested in cases where annotators are sure about their decision. 
\paragraph{Features.} There are two decisions to be made for the features: which features to rate (fluency, adequacy, quality, sensibleness, ... ) and how to rate them (Likert scale, continuous scale, pairwise comparison, ...).

\subsection{Ranking}

%\emph{Spot the Bot} can be interpreted as a tournament amongst bots, whose objective it is to pass as a human. In pairwise, direct conversations, bots compete with each other to achieve this goal. The annotation result is thus based on an utmost direct comparison of each pair of bots. Whichever bot wins most of these direct matches, wins the tournament.
%Intuitively, the bot that is able to fool a human earlier or hide its non-humanity longer, can be considered the better bot. 
We first define a win function for the annotations of the pairwise, direct conversations between the bots. The outputs of the win function are then aggregated to determine the overall winner of the tournament.

\paragraph{Win Function.}

Each annotations at each segment length $s_{ij} = e_0, ... , e_{i_j}$ constitutes the result of one annotation applied by one crowdworker, individually labeling each of the two conversation participants as either \textit{bot, human}, or \textit{unsure}. The winner of segment $s_{ij}$ under a crowdworker's annotation is determined by the following ordering of the labels: $\textit{human} >  \textit{unsure} > \textit{bot}$. That is, if bot $B_i$ is assigned the label \textit{human} and bot $B_j$ has label \textit{bot} or \textit{unsure}, $B_i$ has won the segment.\footnote{This process is repeated for all crowdworkers who annotated the segment - in our case two per segment - and each win is counted separately.} Similar to \newcite{bojar-etal-2013-findings},  we define a win rate of $B_i$ against $B_j$ to aggregate the wins from all annotations of all segments stemming from conversations between $B_i$ and $B_j$, as: 

\begin{equation}\label{eq:win_function}
    \frac{\textsc{wins($B_i$, $B_j$)}}{\textsc{wins($B_i$, $B_j$)} + \textsc{wins($B_j$, $B_i$)}}
\end{equation},
where $\textsc{wins($B_i$, $B_j$)}$ denotes the number of times that $B_i$ wins against $B_j$.  %Since this formula ignores ties, it holds that $\textsc{win-rate($B_i$, $B_j$)} = 1 - \textsc{win-rate($B_j$, $B_i$)}$. 

\paragraph{Ranking.}
To create the ranking, we follow the approach by \newcite{dusek-etal-2018-findings} where the ranking is generated by the TrueSkill algorithm based on the win rate, and significant differences in performance are determined by bootstrap sampling. The result is a ranked set of clusters, where each cluster is composed of algorithms that do not have a significant difference in performance. 

\subsection{Survival Analysis}

We argue in Section \ref{sec:intro} that a bot that stays hidden longer is in a sense better. Therefore we apply survival analysis to assess the time it takes before a bot is spotted.

Survival analysis has a long history in the medical domain, where it is used, for example, to estimate the effect of different treatments on the long term survival rate of patients. In engineering disciplines it can be used for example to estimate the time to failure of certain machine components.

In our case, we are interested in the time, corresponding to the number of exchanges, until a dialogue system is spotted as such. We interpret the data gathered (see Section \ref{sec:setting}) as such: the ``spotted'' event occurred if the system was annotated as ``bot'' and it survived if it was annotated as ``unsure'' or ``human''. Let $N$ be the number of exchanges in the the annotated conversation segment, meaning that each dialog system produced $N$ outputs. If the dialog system was not spotted, then we know it survived for at least $N$ exchanges. For survival analysis this is a so-called right-censored data point. In case the dialogue system was spotted as such, we cannot tell the exact number of exchanges it took for an annotator to spot it, meaning it could have taken less than $N$ exchanges to spot. We therefore record that the spotting event happened in the interval $(0, N]$, a so-called interval-censored event.

From this data we can get non-parametric estimates of the survival function of the different systems per domain \citep{nonparametric_survival_censored}.  To check whether these differences are significant, we apply a generalized log-rank test \citep{generalized_logranktest_zhao2004}.

The field of survival analysis also offers different models for relating various covariates to the survival time. One of the most prominent among such models is the \emph{Cox Proportional Hazards Model} \citep{cox_ph_model}, which we use to study the influence of the different features outlined in Section \ref{sec:setting} on the time before the systems are spotted. This approach lets us compare bots in more detail and offers researchers insight into the reason of their bot's result in our evaluation.  We use the \emph{icenReg} R package \citep{icenReg}, which allows us to fit a Cox model to our interval-censored data.

\section{Experiments}
We apply \emph{Spot The Bot} to three widely used domains to train conversational dialogue systems: PersonaChat \cite{zhang-etal-2018-personalizing}, Dailydialog \cite{li-etal-2017-dailydialog}, and Empathetic Dialogues \cite{rashkin-etal-2019-towards}. For each domain, we prepared a pool of bots to be ranked and analysed. 
%The \emph{Spot The Bot} framework returns two main analyses. First, a ranking of the conversational agents, which differentiates between bots that are significantly different and those where the performance is comparable. Second, the Survival Analysis, which states for each conversational agent the probability of survival after each segment. Furthermore, the Survival Analysis returns which of the features have a significant impact on the performance of the conversational agent. 


\paragraph{Amazon Mechanical Turk.} We recruited paid crowdworkers from Amazon Mechanical Turk (AMT). In order to avoid that the results are biased towards the performance of a few crowdworkers, we designed a HIT as a batch of 20 conversations and each worker was only allowed to work on three batches. We designed the batches so that two segments of the same conversations never appear in the same batch, and each batch contains different segments of different conversations.  

\paragraph{Features.} We chose three features: sensibleness and specificity \cite{adiwardana2020towards}, which are shown to capture the core conversational behaviour of answering sensibly and not with illogical statements, while being specific to the given context of the conversation. The third feature is fluency, which states if the utterances are grammatically correct and fluent. The features are rated by preference ranking, that is, the annotator states which of the two entities performed better with respect to the features. This allows to compute the win-rates on the feature basis which makes comparisons between entities more stable. 


\subsection{Domains}

\begin{table}[h!]
\vspace{-1mm}
\begin{center}
\resizebox{.45\textwidth}{!}{
\begin{tabular}{l||cccl} 
\hline
%\abovespace
\textsc{Domain Name} & \textsc{\#Dialogues} &  \textsc{Avg. Exchanges} &  \textsc{$|B|$} & \textsc{Segments}\\
\hline 
\textsc{Dailydialog}  &     13118   & 3.74 &   4  & 2,3,5 \\
\textsc{Epathetic Dialogues} &    25000   & 1.65 &   5   &  1,2,3   \\
\textsc{Personachat}    & 10907   & 7.85  &   6   &  2,3,5  \\
\hline
\end{tabular}
}
\end{center}
\caption{Overview of the domains}
\label{table:domain-overview}
\end{table}


We apply Spot The Bot on three different domains, which all are based on conversations between two humans (see Table \ref{table:domain-overview} for an overview). Thus, the dialogue systems learn to imitate human conversational behaviour. 

\paragraph{Personachat.} PersonaChat \cite{zhang-etal-2018-personalizing} contains dialogues between two humans, each of the conversation participants is given a predefined persona. The persona is a set of characteristics of a person (name, occupation, hobbies, etc.), and the goal of the conversation is to mimic the process of getting to know each other. 
\paragraph{Dailydialog.} Dailydialog \cite{li-etal-2017-dailydialog} is a dataset that contains dialogues that occur in daily life situations. The data is crawled from English learning websites. Thus, the dialogues are better curated and more formal. Furthermore, the data is annotated with features that represent the emotion in the dialogue. For our experiment, we did not make use of these features. 
\paragraph{Empathetic Dialogues.} Empathetic Dialogues \cite{rashkin-etal-2019-towards} focuses on empathetic response generation. The dialogues occur between two persons that discuss a situation that happened to one of the participants. Thus, there are two types of participants: the speaker and the listener. The first describes the situation and their feelings about it, and the listener responds empathetically. 

\paragraph{Segmentation.} The segment lengths are based on the lengths of the dialogues in a domain. PersonaChat and Dailydialog have longer conversations. Thus, we used segments of 2,3, and 5 exchanges. The Empathetic Dialogue domain has rather short dialogues, thus, we used segment lengths of 1,2, and 3 exchanges. We noted that having too long conversations leads to more cognitive strain on the annotators. 

\subsection{Dialogue Systems}
For the PersonaChat domain, we prepared a pool of $|B| = 6$ different dialogue systems: The Blender\footnote{Due to GPU restrictions, we used the small model with 90M parameters.} system (BL) \cite{roller2020recipes}, which is a state-of-the-art dialogue system trained on different domains, Lost in Conversation\footnote{\url{https://github.com/atselousov/transformer\_chatbot}} (LC) and Huggingface~\footnote{\url{https://github.com/huggingface/transfer-learning-conv-ai}}(HF), which were the top rated dialogue systems in the ConvAI2 challenge \cite{dinan2020convai2}, KVMemNN (KM), which is the baseline used for the ConvAI2 challenge, and three systems trained with ParlAI \cite{miller2017parlai}: BertRank (BR), and a small Seq2Seq model (DR). The small Seq2Seq model is trained only for 3 epochs, and returns only very general and short answers, which serves to assess how Spot The Bot works for very weak models.  \\
For the Empathetic Dialogues domain, we used a set of $|B| = 5$ dialogue systems using the ParlAI framework: The Blender model (BL), a fine-tuned GPT2 (GPT), BertRank (BR), Seq2Seq+Att (S2), and a small Seq2Seq model (DR).  \\
For the Dailydialog domain, we used the same set of systems as the Empathetic Dialogues, except for the Blender model.



\subsection{Ranking Results}
\input{tables/win_rates/full_win_rate}
Table \ref{tab:win_rates} gives an overview of the win rates for each pair of dialogue systems as well as the ranking ranges, which denote the clusters. The significance is computed by the Chi-square test. For each domain, most pairwise win-rates are significant. \\
As expected DR performs worst in all three domains, which is due to its repetitive nature, which is exposed over the course of a dialogue. In the Dailydialog domain and the Empathetic Dialogues domain, the GPT2 and the BR models perform equally, i.e. they end up in the same cluster. In both domains, the language models outperform the S2 model, which is in line with the expectation of large and powerful language models outperforming previous state-of-the-art models.
The BL model outperforms all other models in both the PersonaChat and Empathetic Dialogues domains, which is in line with the results presented by the authors of the Blender model. The pairwise comparison of BL to the LC model is not significant in our case, which is probably due to the fact that we used the small version of the BL model. Furthermore, the LC model is ranked very highly. This corresponds to the findings of the ConvAI2 challenge. However, in Spot The Bot, the KV is ranked much higher than the Huggingface model, which is not in line with the ConvAI2 evaluation. 


\subsection{Survival Analysis}

% figure
%----------------------------------------------------------------------------
\begin{figure*}[h!]
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/survival/dailydialog_all.png}
    \caption{Dailydialog}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/survival/empathetic_all.png}
    \caption{Empathetic Dialogues}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/survival/convai2_all.png}
    \caption{PersonaChat}
  \end{subfigure}
  \caption{Survival function per system estimated for each domain.}
  \label{fig:survival_all_domains}
\end{figure*}

Figure \ref{fig:survival_all_domains} shows the survival functions for the three domains. We see that there are differences in the survival of the different systems that correspond to the fooling rates observed in Figure \ref{fig:fooling_rate}. The survival rates imply the same rankings as those from pairwise win-rates reported in Table \ref{tab:win_rates}. The only exception being the Empathetic Dialogues domain, where GPT and BR switch places. Importantly, the distinction between these two is not significant in both rankings. Further non-significant differences are S2 and DR in the Empathetic Dialogues domain, BR and S2 in the Dailydialog domain, and LC and KV in the PersonaChat domain. All other pairwise comparisons of survival curves are significant with $p  < 0.05$ after correction for multiple comparisons.

\begin{table}[ht]
    \centering
    \small
    \input{tables/survival/features}
    \caption{Per feature win-rate of the different systems over all domains. Bold numbers indicate that the feature has a significant
    influence on system survival according to a Cox model.}
    \label{tab:survival_features}
\end{table}

\paragraph{Feature Influence} For each of the three features -- fluency, specificity, and sensibleness -- annotators have to specify whether one entity performed better, the same, or worse than the other. We encode this information as $1$, $0$, and $-1$ respectively and fit a Cox proportional hazards model for every system independently with the features as covariates.

The numerical entries in Table \ref{tab:survival_features} refer to the per-feature win-rate of each bot, which is computed analogously to Equation \ref{eq:win_function} using the feature annotations directly. Using these can be more intuitive than relying on the magnitude of the model coefficients, which can be hard to interpret.
Bold entries in Table \ref{tab:survival_features} show which features have a significant influence on the system being spotted. All significant effects go in the intuitive direction, meaning that a higher feature value leads to longer survival.  For example, for the DR model, the fluency feature is significant across all three domains, and together with its low fluency win-rate, we can deduce that it is often spotted due to its low fluency. Sensibleness seems to be an important feature across the board, meaning that in general, bots can be spotted due to inappropriate, nonsensical answers or hide if they respond in a suitable manner. Interestingly, specificity seems to be mostly unimportant which could be due to either the bots not being noticeably unspecific, or it being an irrelevant feature for the chosen domains.

\section{Discussion}

\subsection{On Reliability}
One key requirement to an evaluation procedure is that repeated executions of the procedure result in the same outcome. We measure how many pairwise conversations between two bots are needed to guarantee a stable ranking. That is, what is the lower bound to $|S_{ij}|$ so that the ranking is stable. For each $|S_{ij}| \in \{3 ... 44\}$, we randomly sample $|S_{ij}|$ conversation for each pair and compute the ranking. We repeat this subsampling procedure a 1000 times, and measure the minimum $|S_{ij}|$ that guarantees the same ranking in at least $95\%$ of cases.
% figure
%----------------------------------------------------------------------------
\begin{figure}[h!]
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/rankig-sigificance.pdf}
    \caption{Stability Experiment.}
    \label{fig:ranking-significance}
  \end{subfigure}
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{figures/rankig-sigificance-loo.pdf}
    \caption{Leave-one-out Experiment.}
    \label{fig:ranking-significance-loo}
  \end{subfigure}
  \caption{Ranking stability experiments.The x-axis denotes the number of pairwise conversations between two bots. The y-axis denotes the rate at which the same ranking is achieved across 1000 repetitions. The horizontal line denotes the 95\% mark. In the lower Figure, we show the experiments for the PersonaChat domain, when leaving one system out.}
\end{figure}
%----------------------------------------------------------------------------
Figure \ref{fig:ranking-significance} shows for each $|S_{ij}| \in \{3 ... 44\}$ the percentage of times in which the most frequent ranking occurred. For the Dailydialog domain $|S_{ij}| = 33$ pariwise conversations are enough to guarantee a stable ranking. In the other two domains this value is reached with over 40 pairwise dialogues. This is due to the fact that in these two domains there are dialogue systems, which are not distinguishable in their performance. In order to investigate this phenomena, we repeat the above experiment on the PersonaChat domain by leaving one system out. Figure \ref{fig:ranking-significance-loo} shows the result of the ranking stability for each left-out system. When leaving one between LC or KV out, the stability is achieved with 25 pairwise dialogues, which is due to the fact that their pairwise comparison is not significant. However, a subsample of their pairwise comparisons might yield significant differences, which leads to different clusters in the ranking. In fact, the two rankings differ in that the first puts KV and LC in the same cluster, and in the second, LC and KV are in separate clusters, with LC being on top.



\subsection{Annotator (Dis)agreement as a Feature}
The robustness of evaluation of conversational dialogue systems is often hampered by inter-annotator agreement (IAA) (cite). % IAA is generally measured using some agreement metric, and the soundness of the evaluation data is determined by segmenting the range of such metrics into (non-)acceptable subranges. 
On the one hand, measuring and reporting IAA is, unfortunately, not a standard practise in the evaluation of chat bots (cite). On the other hand, it has been shown that producing annotations with high IAA on open domain conversations is a challenging endeavour, as annotating features on e.g.\ likert scales is prone to be impeded by subjective interpretation and idiosyncratic annotator behaviour (cite).
%The main goal for a bot in our setting is to disguise as a human, and the bot which manages to do so the longest, wins. 

In our setting, annotator disagreement on a bot's human-like behaviour can be interpreted as a feature: A bot that manages to fool one of two annotators (maximal disagreement) into believing it is human can be said to have performed better than a bot that does not manage to fool any annotator (maximal agreement). Conversely, a bot that is spotted with high agreement has performed worse than a bot that is spotted with lower agreement.
% Based on this intuition, we apply two measures of inter-annotator agreement, i.e.\ a) direct agreement between the annotated labels (human, bot, undecided) per entity, and b) agreement on the outcome of the win function (as defined above).

%In a first step, we calculate label agreement by the fraction of segments where an entity was annotated with the same label by both annotators. Table \ref{tab:label_agreement_dailydialog} shows the label agreement for the Dailydialog domain per segment length and bot. Random chance agreement is 0.33, given three labels. We see that all agreements are higher than agreement by chance and that agreement increases with the increase of the segment length, indicating that segment length plays an important role in the annotation task regarding agreement. Furthermore, we see that the bots that perform the weakest regarding fooling rates and ranking ({S2} and {D}) reach high agreement, i.e.\ for system {D}, its agreement is almost on par with the one for human entities.

%\begin{table}[ht]
%    \centering
%    \begin{tabular}{l|ccc|c}
%    \toprule
% & 2 & 3 & 5 & {Overall}\\ \hline
%\it human & \it 0.80 & \it 0.74 & \it 0.82 & \it 0.78\\ \hline
%BR & 0.54 & 0.62 & 0.70 & 0.62\\
%HF & 0.44 & 0.53 & 0.56 & 0.51\\
%S2S & 0.56 & 0.66 & 0.72 & 0.64\\
%D & 0.69 & 0.73 & 0.87 & 0.76\\
%\bottomrule
%    \end{tabular}
%    \caption{Label agreement per segment length in the Dailydialog domain.}
%    \label{tab:label_agreement_dailydialog}
%\end{table}

%This analysis tells us how well annotators agree in general, but does not indicate whether they agree on the level of human-like behaviour of each bot. 
To investigate this, we calculate per bot and label the percentage of cases where both annotators annotate the label if one of them does. Given three labels (\emph{human, bot, unsure}), the chance for random agreement is 0.33. Results are shown in Table \ref{tab:agreement_per_label}.

\input{tables/annotator_agreement}

A bot's goal in this analysis is to reach high agreement on the \emph{human} label (since it has then tricked both annotators into believing it is human) and low agreement on the \emph{bot} label (it is better to be spotted as a bot by only one annotator). 
We observe that the bots that rank high in the tournament outcome and the survival analysis (BL, HF, LC) obtain the highest agreement on the \emph{human} label and lowest agreement on the \emph{bot} label in all domains. For the BL system in the Empathetic Dialogues domain, the agreement values are highly distinct from the competitors and close to the agreement that the human entities obtain. That is, in 72\% of the cases where one annotator labels the bot as being human, the other annotator agrees. Conversely, the annotators agree that BL is a bot in only 33\% of the cases where one annotator identifies it as a bot. That is, the agreement for the \emph{human} label is more than twice as high as agreement by chance, and agreement on the \emph{bot} label is as low as random agreement. On the other end, we see that the DR system consistently obtains highest agreement when being identified as a bot, and lowest when it is perceived as a human. 
%The Dailydialog domain seems to be the hardest in terms of convincing both annotators of human-like behaviour, i.e.\ the agreement on the \emph{human} label for the best performing bot (HF) is only slightly higher than random agreement. 
Our analysis of the agreement on the assigned labels suggests that the ranking in our Spot The Bot experiments do not result from random agreement between the annotators, i.e.\ the annotations of the best and worst performing systems show agreements distinctly higher than chance regarding the respective labels. 
%We see that {HF} reaches the best agreement on the \emph{human} label and the lowest on the \emph{bot} label. That is, if one annotators annotates it as being human, the other annotator does so as well in 38\% of the cases, and in 64\% percent of the cases where one annotator spots {HF} as a bot, the other annotator agrees. Conversely, system {D} again has the highest agreement regarding the \emph{bot} label and the lowest on the \emph{human} label. Overall, there is little agreement on the unsure label, indicating that a decision can be taken for most segments.

%\begin{table*}[ht!]
%    \centering
%    \small
%    \begin{tabular}{l|cccccc}
%    \toprule
%Bot & bot-bot & bot-human & bot-unsure & human-human & human-unsure & %unsure-unsure\\ \hline
%\it human & \it 0.04 & \it 0.16 & \it 0.03 & \it 0.74 & \it 0.03 & \it 0.00\\ %\hline
%BR & 0.54 & 0.24 & 0.11 & 0.07 & 0.02 & 0.01\\
%HF & 0.39 & \bf 0.30 & 0.13 & \bf 0.11 & 0.06 & 0.01\\
%S2S & 0.59 & 0.21 & 0.13 & 0.05 & 0.01 & 0.01\\
%D & \bf 0.74 & 0.13 & 0.08 & 0.01 & 0.03 & 0.01\\
%\bottomrule
%    \end{tabular}
%    \caption{Distribution over annotated label pairs for the Dailydialog %domain. }
%    \label{tab:label_pair_percentage}
%\end{table*}

%We extend this analysis to the level of conflicting label pairs.\footnote{Since we always have two annotators per segment, we obtain pairs of labels.} A label pair is the combination of the labels assigned to an entity in a segment. We calculate the distribution over label pairs found in the annotation per entity, shown in Table \ref{tab:label_pair_percentage}. We are here particularly interested in conflicting labels (\emph{bot-human, bot-unsure, human-unsure}). Since \emph{human-unsure} is quite rare, we focus on the other two. The most erratic label pair in terms of disagreement is the \emph{bot-human} label, indicating a vastly different impression of the bots' behaviour in the eyes of the annotators. We see that it is almost twice as frequent as the \emph{bot-unsure} label. Overall however, this problematic label confusion occurs in at most 30\% of the cases (HF), leaving the other 70\% with less severe disagreement.

\subsection{On Stability against weak Annotators}
One drawback of Likert-scale based evaluation methods is that many annotations need to be removed due to unreliable annotators \cite{lowe-etal-2017-towards}. \emph{Spot The Bot} shows that it is stable with respect to weak annotators. Since we can measure how often the annotators correctly classify an entity, we can rate the quality of an annotator. A random annotator would receive a correctness rate of 50\%. Table \ref{table:annotator-overview} shows an overview of the annotators for each domain. 

\input{tables/annotator_overview}

 The average correctness score is significantly higher than random. For the Dailydialog and Empathetic Dialog domain, the rate of annotators, which achieved a rate below 50\% was below 10\% of all annotators. For the PersonaChat domain the rate is higher, which is due to the fact that stronger dialogue systems were in the pool of bots. The average correctness scores for predicting humans correctly, is high for all domains. 
\emph{Spot The Bot} proves to be stable against annotators with low scores. When removing all annotators with scores below $75\%$ the rankings remain stable. Only the significance scores decrease as a large amount of dialogues gets removed. 

\subsection{On Cost and Time efficiency}
Evaluation methods, which are costly and take up a long time slow down the development cycle of dialogue systems. \emph{Spot The Bot} brings down the cost and time effort compared to other methods. 
\input{tables/annoation-cost}
In Table \ref{table:annotator-cost} the median time per annotation is displayed.  For the Dailydialog and PersonaChat domain, the average annotation time is at around 25 seconds. For the Empathetic Dialogues it's at 18 seconds, which is due to the smaller segments. We compare this to the time to create conversations between humans and bots. We recruited three dialogue system experts from our lab to interact with the systems. Each expert interacted for 5 times with each system. The average times do not take into account the time needed to instruct the experts. For the Dailydialog and Empathetic Dialogies domains, it takes over 2 Minutes per conversation. For PersonaChat, the time increased to almost 4 minutes. Thus, \emph{Spot The Bot} drastically increases the annotation speed, while reducing the mental strain for the human raters.



\subsection{Segment Length Analysis}
\input{tables/win_rates/segments}
The intuition behind the segment length is that if the dialogue is too long then most conversational dialogue systems will always be exposed as such. Contrary, if the dialogues are too short then there is too little information to discriminate between dialogue systems. Thus, having different lengths of conversations ensures that these extremes do not occur. The effect is shown in Table \ref{tab:segment-analysis}. For each dialogue system the rate at which it is classified as human is depicted for the three different segments. For each of the dialogue systems this rate goes down, which is in line with our intuition. Similarly, the rate of unsure classification is lower at later segments. In later segments two phenomena occur. First, the number of ties increases, as most dialogue systems get exposed as such, the number of ties in the Dailydialog domain increases from 72\% to 81\%. Second, the difference between the win-rates increases. That is, better bots have a higher win-rate and the lower ranked bots get a lower win rate. However, the win-rates are less significant due to the high number of ties. For instance, the GPT model increases its win rate to 0.81, whereas the win rate for S2 decreases from 0.46 to 0.34.


\section{Conclusion and Future Work}
Points relevant for discussion of related work:

\begin{itemize}
    \item We present an evaluation framework for dialogue system that produces reliable, repeatable results in an efficient manner. The framework introduces bot-bot conversations as a means to bypass creating human-bot conversations which are costly to obtain and often of sub-par quality (cf.\ convai2 paper; they threw away a large portion of their data because they were of low quality). It is generally hard to do human-bot convos, as the humans need to be instructed on how to behave. Furthermore, the bot-bot talks allow to evaluate the bots behaviour over time, which is different than the evaluations, which work on a fixed context and a single bot-response.
    \item The framework requires fewer annotated conversations than related approaches to generate robust results and is thus quicker and more cost-effective to run. From empirical evidence, it seems that around 40 pairwise conversations are enough.
    \item We show that disagreement between human annotators is seen as an indicator of the strength of a bot's performance, as opposed to low agreement constituting an issue in related approaches.
    Intuition: a bot where everyone agrees that it is a bot, is worse than a bot where most rate it as a bot but with less agreement.
    \item We apply the framework to three well-known domains as well as common baselines to produce a stable ranking among them.
\end{itemize}

*TODO combine the conducted multifaceted-evaluation of the bots into an overall assessment of their performance in Spot the Bot. We need to show how all of these analyses are integrated in a digest-able overview.



\begin{itemize}
    \item AutoJudge, try to automate this process
    \item Analyse the granularity, how much difference can this procedure pick up?
    \item Apply to Lifelong learning or application to bot-development process to measure progress. How can this be done.
\end{itemize}


\section*{TODO: integrate}
\begin{itemize}
    \item bullet point introduction
    \item discuss (our) efforts in creating human-bot convos: how long did it take us?
    \item Motivate better the use of win rates vs survival analysis $\rightarrow$ point to interpretability of features in survival analysis, early on in the paper (strong point of the paper)
    \item motivate: why is bot-bot better than self-talk? we haven't given a reason, yet. $\rightarrow$ intuitive: you can directly compare bots. ACUTEval shwos self-talk logs of two bots next to eachother, however it is hard to compare them. Thus, having two bots in the same conversation is a better ground for comparison.
    \item Problem that annotators realize that entities in convos are either both humans or both bots, i.e. there are not human-bot conversations $\rightarrow$ measure ratio "sameness" of annotations (both entities are annotated as humans or both as bots; overall and during annotation job)
    \item Human-bot convos: show that excluding human-bot convos does not change ranking, but reduces point differences between the bots $\rightarrow$ Show barplot of point w/o human-bot convos?
    \item analyze/show time per annotation job per annotator? is it an important factor? $\rightarrow$ spammers? at least discuss how we blacklist spammers
    \item raw fooling rates vs avg of avg fooling rates per bot? (against annotator bias)
    \item comparison to Convai2 evaluation approach and ranking. Comparison to ACUTE-Eval.
    \item appedix: show some human-human convos that are anntoated as bots

\end{itemize}

\section*{Acknowledgments}

\bibliography{emnlp2020}
\bibliographystyle{acl_natbib}
\newpage
\appendix

\section{Annotation Tool}
Figure \ref{fig:tool} shows the annotation tool. The annotator is presented with a segment of the conversation, that is, with the first $i$ exchanges. In a first step, the annotator needs to decide for both entities separately if they are human or not. In the cases where it is not yet possible to decide, the annotator can choose to state that they are undecided. In the second step, the annotators are asked to state which of the two entities performs better with respect to three different features: fluency, sensibleness, and specificity with following definitions:
\begin{itemize}
    \item Fluency: Which entities' language is more fluent and grammatically correct?
    \item Sensibleness: Which entities' responses are more sensible? If the answer seems confusing, illogical, contradictory, or factually wrong then it is NOT sensible.
    \item Specificity: Which entities' responses are more specific and explicit in the given context? An answer is specific if it can be given only in the current context.
\end{itemize}

\label{sec:appendix}
\begin{figure*}[!ht]
	\begin{center}
        \begin{tabular}{@{}c@{}}
		\includegraphics[width=0.45\textwidth]{figures/AnnotationTool1.png} 
		\includegraphics[width=0.45\textwidth]{figures/AnnotationTool2.png}
       \end{tabular}
	\end{center}\vspace{-3mm}
	\captionof{figure}{The annotation tool. Left is the decision about the nature of each entity. Right is the decision with regard to the features. }
\label{fig:tool}
\end{figure*}

\section{Gamification}
\label{sec:appendix_gamification}
As an alternative to the segmentation approach, we experimented with a gamified version of the annotion tool (see Figure \ref{fig:game-tool}). In this version the annotators were presented with the first turn of the conversation. At each point in time they could choose whether to open the next turn or decide for an entity. If both decisions have been made the annotators had to decide for the three aforementioned features, which entity performs better. 
The task was framed as a game and the annotators received feedback in the form of a leaderboard. The score was a combination of the correctness (were the entities classified correctly) and a turn-penalty, that is, the more turns they opened the lower the score. As an additional incentive, the winner was awarded a bonus payment. 
However, this approach resulted in unwanted behaviour of the annotators. There were some that always decided after just one exchange, which lead to random annotations. Others opened the whole conversation first, and then decided. To counteract these behaviours the tool needed a lot of fine-tuning, which made the approach not reliable for practical use.


\label{sec:appendix}
\begin{figure*}[!ht]
	\begin{center}
        \begin{tabular}{@{}c@{}}
		\includegraphics[width=0.45\textwidth]{figures/GameAnnotationTool1.png} 
		\includegraphics[width=0.45\textwidth]{figures/GameAnnotationTool2.png}
       \end{tabular}
	\end{center}\vspace{-3mm}
	\captionof{figure}{Gamified version of the annotation tool. }
\label{fig:game-tool}
\end{figure*}

\section{Experimental Setup}
\label{app:experiments}
All the systems, which we used were trained using the ParlAI system. For the Lost in Covnersation systen, Blender, Huggingface system, and the KVMemNN, we used the available models. The other systems were trained using the ParlAI training functionality with the following hyperparameters. We trained all the models for 30 epochs.
For all the Bert-Rank experiments, we used the Bi-Encoder and optimized the last four layers due to GPU restrictions. 
The GPT2 models were trained with the standard setting. Due to GPU restrictions, we used the small version of the GPT2 model. 
The sequence-to-sequence model was trained with two layers of GRUs \cite{cho2014gru} each with 512 hidden units. We used the general attention mechanism \cite{luong-etal-2015-effective} and used the FastText word-embeddings\cite{bojanowski-etal-2017-enriching}. We used the Adam optimizer \cite{kingma2014adam} with a learning rate of 0.001. 
For the small sequence-to-sequence model, we used a one layer GRU with 128 hidden units. We trained this model for only 3 epochs as we noted that after three epochs it is able to generate the generic answers. 

\section{Feature Rankigns}
\input{tables/win_rates/fluency_win_rate}

In Table \ref{tab:fl-win-rates} the win rates and rankings for the fluency feature are shown. For the PersonaChat domain the ranking differs significantly form the bot detection, as KV, LC, BR, and HF are all in the same cluster. 
\input{tables/win_rates/ssa_win_rate}
In Table \ref{tab:ssa-win-rates} the win rates for the Sensibleness and
Specificity Average (SSA) are shown. A system wins if it is favoured both in sensibleness and specificity. The rankings are similar to the bot detection rankings. For empathetic dialogues, the GPT model performs indistinguishably from the S2 model. In the PersonaChat domain HF and BR are in the same cluster. 



\section{Fooling Rates}
% figure
%----------------------------------------------------------------------------
\begin{figure*}[h!]
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/dailydialog/fooling_rates_over_time.png}
    \caption{Dailydialog}
    \label{fig:1}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/empathetic/fooling_rates_over_time.png}
    \caption{Empathetic Dialogues}
    \label{fig:2}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/personachat/fooling_rates_over_time.png}
    \caption{PersonaChat}
    \label{fig:2}
  \end{subfigure}
  \caption{Fooling Rates for each domain. }
  \label{fig:fooling_rate}
\end{figure*}
%----------------------------------------------------------------------------

\end{document}
